## ğŸš£â€â™‚ï¸ ä½¿ç”¨ PaddleNLP åœ¨ Intel HPU ä¸‹è·‘é€š llama2-7b æ¨¡å‹ ğŸš£
PaddleNLP åœ¨ IntelÂ® GaudiÂ®2D([äº†è§£ Gaudi](https://docs.habana.ai/en/latest/index.html))ä¸Šå¯¹ llama2-7B æ¨¡å‹è¿›è¡Œäº†æ·±åº¦é€‚é…å’Œä¼˜åŒ–ï¼Œä¸‹é¢ç»™å‡ºè¯¦ç»†å®‰è£…æ­¥éª¤ã€‚

##  ğŸš€ å¿«é€Ÿå¼€å§‹ ğŸš€

### ï¼ˆ0ï¼‰åœ¨å¼€å§‹ä¹‹å‰ï¼Œæ‚¨éœ€è¦æœ‰ä¸€å° Intel Gaudi æœºå™¨ï¼Œå¯¹æ­¤æœºå™¨çš„ç³»ç»Ÿè¦æ±‚å¦‚ä¸‹ï¼š

 | èŠ¯ç‰‡ç±»å‹ | å¡å‹å· | é©±åŠ¨ç‰ˆæœ¬ |
 | --- | --- | --- |
 | Gaudi | 225D | 1.17.0 |


### ï¼ˆ1ï¼‰ç¯å¢ƒå‡†å¤‡ï¼š(è¿™å°†èŠ±è´¹æ‚¨5ï½15min æ—¶é—´)
1. æ‹‰å–é•œåƒ
```
# æ³¨æ„æ­¤é•œåƒä»…ä¸ºå¼€å‘ç¯å¢ƒï¼Œé•œåƒä¸­ä¸åŒ…å«é¢„ç¼–è¯‘çš„é£æ¡¨å®‰è£…åŒ…
docker pull vault.habana.ai/gaudi-docker/1.17.0/ubuntu22.04/habanalabs/pytorch-installer-2.3.1:latest
```
2. å‚è€ƒå¦‚ä¸‹å‘½ä»¤å¯åŠ¨å®¹å™¨
```
docker run -it --runtime=habana -e HABANA_VISIBLE_DEVICES=all -e OMPI_MCA_btl_vader_single_copy_mechanism=none --cap-add=sys_nice --net=host --ipc=host vault.habana.ai/gaudi-docker/1.17.0/ubuntu22.04/habanalabs/pytorch-installer-2.3.1:latest
```
3. å®‰è£… paddle
```
# paddlepaddleã€é£æ¡¨ã€æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæä¾›è¿ç®—åŸºç¡€èƒ½åŠ›
pip install paddlepaddle==0.0.0 -f https://www.paddlepaddle.org.cn/whl/linux/cpu-mkl/develop.html
```
4. å®‰è£… paddleCustomDevice
```
# paddleCustomDeviceæ˜¯paddlepaddleã€é£æ¡¨ã€æ·±åº¦å­¦ä¹ æ¡†æ¶çš„è‡ªå®šä¹‰ç¡¬ä»¶æ¥å…¥å®ç°ï¼Œæä¾›Intel HPUçš„ç®—å­å®ç°ã€‚
git clone --recursive https://github.com/PaddlePaddle/PaddleCustomDevice
cd PaddleCustomDevice
git submodule sync
git submodule update --remote --init --recursive
cd backends/intel_hpu/
mkdir build && cd build
cmake ..
make -j8
pip install dist/paddle_intel_hpu-0.0.1-cp310-cp310-linux_x86_64.whl
```
5. å…‹éš† PaddleNLP ä»“åº“ä»£ç ï¼Œå¹¶å®‰è£…ä¾èµ–
```
# PaddleNLPæ˜¯åŸºäºpaddlepaddleã€é£æ¡¨ã€çš„è‡ªç„¶è¯­è¨€å¤„ç†å’Œå¤§è¯­è¨€æ¨¡å‹(LLM)å¼€å‘åº“ï¼Œå­˜æ”¾äº†åŸºäºã€é£æ¡¨ã€æ¡†æ¶å®ç°çš„å„ç§å¤§æ¨¡å‹ï¼Œllama2-7Bæ¨¡å‹ä¹ŸåŒ…å«å…¶ä¸­ã€‚ä¸ºäº†ä¾¿äºæ‚¨æ›´å¥½åœ°ä½¿ç”¨PaddleNLPï¼Œæ‚¨éœ€è¦cloneæ•´ä¸ªä»“åº“ã€‚
git clone https://github.com/PaddlePaddle/PaddleNLP.git
cd PaddleNLP
python -m pip install -r requirements.txt
python -m pip install -e .
```

### ï¼ˆ2ï¼‰æ¨ç†ï¼š(è¿™å°†èŠ±è´¹æ‚¨10~15min æ—¶é—´)
1. å•å¡æ¨ç†

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤è¿›è¡Œæ¨ç†ï¼š
```bash
python inference_hpu.py
```

æˆåŠŸè¿è¡Œåï¼Œå¯ä»¥æŸ¥çœ‹åˆ°æ¨ç†ç»“æœçš„ç”Ÿæˆï¼Œæ ·ä¾‹å¦‚ä¸‹ï¼š
```
[2024-10-25 02:42:42,220] [    INFO] - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load 'meta-llama/Llama-2-7b-chat'.
[2024-10-25 02:42:42,427] [    INFO] - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b-chat'.
[2024-10-25 02:42:42,427] [    INFO] - Loading configuration file /root/.paddlenlp/models/meta-llama/Llama-2-7b-chat/config.json
[2024-10-25 02:42:42,428] [    INFO] - Loading weights file from cache at /root/.paddlenlp/models/meta-llama/Llama-2-7b-chat/model_state.pdparams
[2024-10-25 02:43:32,871] [    INFO] - Loaded weights file from disk, setting weights to model.
[2024-10-25 02:44:15,226] [    INFO] - All model checkpoint weights were used when initializing LlamaForCausalLM.

[2024-10-25 02:44:15,226] [    INFO] - All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-chat.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[2024-10-25 02:44:15,229] [    INFO] - Loading configuration file /root/.paddlenlp/models/meta-llama/Llama-2-7b-chat/generation_config.json

['myself. I am a 35 year old woman from the United States. I am a writer and artist, and I have been living in Japan for the past 5 years. I am originally from the Midwest, but I have lived in several different places around the world, including California, New York, and now Japan.\nI am passionate about many things, including art, writing, music, and travel. I love to explore new places and cultures, and I am always looking for new inspiration for my art and writing. I am also a big fan of Japanese culture, and I try to learn as much']
```
2. å¤šå¡æ¨ç†

æ‰§è¡Œå¦‚ä¸‹å‘½ä»¤è¿›è¡Œæ¨ç†ï¼š
```bash
bash test_llama_2x.sh
```
æˆåŠŸè¿è¡Œåï¼Œå¯ä»¥æŸ¥çœ‹åˆ°æ¨ç†ç»“æœçš„ç”Ÿæˆï¼Œæ ·ä¾‹å¦‚ä¸‹ï¼š
```bash
[2024-10-29 11:24:39,468] [    INFO] - We are using <class 'paddlenlp.transformers.llama.tokenizer.LlamaTokenizer'> to load 'meta-llama/Llama-2-7b-chat'.
[2024-10-29 11:24:40,705] [    INFO] distributed_strategy.py:214 - distributed strategy initialized
I1029 11:24:40.706755 14711 tcp_utils.cc:181] The server starts to listen on IP_ANY:59129
I1029 11:24:40.706897 14711 tcp_utils.cc:130] Successfully connected to 127.0.0.1:59129
[2024-10-29 11:24:42,740] [    INFO] topology.py:357 - Total 2 pipe comm group(s) create successfully!
[2024-10-29 11:24:52,064] [    INFO] topology.py:357 - Total 2 data comm group(s) create successfully!
[2024-10-29 11:24:52,064] [    INFO] topology.py:357 - Total 1 model comm group(s) create successfully!
[2024-10-29 11:24:52,065] [    INFO] topology.py:357 - Total 2 sharding comm group(s) create successfully!
[2024-10-29 11:24:52,065] [    INFO] topology.py:279 - HybridParallelInfo: rank_id: 0, mp_degree: 2, sharding_degree: 1, pp_degree: 1, dp_degree: 1, sep_degree: 1, mp_group: [0, 1],  sharding_group: [0], pp_group: [0], dp_group: [0], sep:group: None, check/clip group: [0, 1]
[2024-10-29 11:24:52,067] [    INFO] - We are using <class 'paddlenlp.transformers.llama.modeling.LlamaForCausalLM'> to load 'meta-llama/Llama-2-7b-chat'.
[2024-10-29 11:24:52,067] [    INFO] - Loading configuration file /root/.paddlenlp/models/meta-llama/Llama-2-7b-chat/config.json
[2024-10-29 11:24:52,068] [    INFO] - Loading weights file from cache at /root/.paddlenlp/models/meta-llama/Llama-2-7b-chat/model_state.pdparams
[2024-10-29 11:25:43,202] [    INFO] - Starting to convert orignal state_dict to tensor parallel state_dict.
[2024-10-29 11:25:45,125] [    INFO] - Loaded weights file from disk, setting weights to model.
[2024-10-29 11:26:04,008] [    INFO] - All model checkpoint weights were used when initializing LlamaForCausalLM.
[2024-10-29 11:26:04,008] [    INFO] - All the weights of LlamaForCausalLM were initialized from the model checkpoint at meta-llama/Llama-2-7b-chat.
If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.
[2024-10-29 11:26:04,010] [    INFO] - Loading configuration file /root/.paddlenlp/models/meta-llama/Llama-2-7b-chat/generation_config.json

['myself\nHello everyone my name is [Your Name], and I am a new member of this community']
I1029 11:26:16.184163 14767 tcp_store.cc:293] receive shutdown event and so quit from MasterDaemon run loop
LAUNCH INFO 2024-10-29 11:26:17,186 Pod completed
LAUNCH INFO 2024-10-29 11:26:17,186 Exit code 0
```
